{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "670a9228-02fa-4532-89f3-4df5446724ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìì Notebook Databricks : merger les CSV et cr√©er une table Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0faaeb36-1b9d-47d2-bc3f-b070c3653d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîß √âtape 1 ‚Äì Config (stockage et chemin CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fce9443-aefc-4d66-9ff4-91940940de47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"comptestockage001\"\n",
    "container_name = \"adfv2tutorial\"\n",
    "conf_key = f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\"\n",
    "\n",
    "# R√©cup√©ration de la cl√©\n",
    "storage_key = dbutils.secrets.get(scope=\"blobstorage_scope\", key=\"key3\")\n",
    "\n",
    "# Configuration Spark\n",
    "spark.conf.set(conf_key, storage_key)\n",
    "\n",
    "# Chemin vers le conteneur\n",
    "wasbs_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\"\n",
    "\n",
    "# Chemin vers les fichiers CSV (tu peux adapter le dossier)\n",
    "csv_folder = wasbs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9255d759-94e4-41cf-884c-41667be5e121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìÇ √âtape 2 ‚Äì Lire et fusionner les fichiers .csv\n",
    "## 2.0 Contr√¥le de sch√©ma avant fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559b4d74-7096-4a40-a97a-63dde513f76a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lister tous les fichiers .csv dans le dossier\n",
    "files = dbutils.fs.ls(csv_folder)\n",
    "csv_files = [f.path for f in files if f.path.endswith(\".csv\")]\n",
    "\n",
    "print(f\"{len(csv_files)} fichiers CSV trouv√©s\")\n",
    "\n",
    "# Lire tous les fichiers et v√©rifier la coh√©rence des sch√©mas\n",
    "schemas = {}\n",
    "df_all = None\n",
    "\n",
    "for path in csv_files:\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(path)\n",
    "    current_schema = set((field.name, field.dataType.simpleString()) for field in df.schema.fields)\n",
    "    \n",
    "    # Stocker le sch√©ma du premier fichier comme r√©f√©rence\n",
    "    if df_all is None:\n",
    "        reference_schema = current_schema\n",
    "        df_all = df\n",
    "        schemas[path] = \"OK\"\n",
    "    else:\n",
    "        if current_schema == reference_schema:\n",
    "            df_all = df_all.unionByName(df)\n",
    "            schemas[path] = \"OK\"\n",
    "        else:\n",
    "            schemas[path] = \"SCH√âMA NON CONFORME\"\n",
    "            print(f\" Fichier ignor√© (sch√©ma diff√©rent) : {path}\")\n",
    "\n",
    "# R√©sum√© des statuts de fichiers\n",
    "print(\"\\nR√©sum√© de l'analyse des sch√©mas :\")\n",
    "for file_path, status in schemas.items():\n",
    "    print(f\"- {file_path} : {status}\")\n",
    "\n",
    "# Afficher les donn√©es fusionn√©es\n",
    "if df_all is not None:\n",
    "    display(df_all)\n",
    "else:\n",
    "    print(\"Aucun fichier valide n‚Äôa √©t√© fusionn√©.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "036b0ec5-a85f-45d0-a23c-a0d9671b30f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# √âtape 3 ‚Äì Sauvegarde dans une table Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7b2b3f8-f4bb-433e-b750-422b69707c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_all.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"table_csv_fusionnee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7338853-65b7-41c8-9bea-d4421ca8125e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TP 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
